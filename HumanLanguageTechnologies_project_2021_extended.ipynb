{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: multiprocess in /storagenfs/g.pisciotta1/.local/lib/python3.8/site-packages (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /storagenfs/g.pisciotta1/.local/lib/python3.8/site-packages (from multiprocess) (0.3.4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/lib/python3.8/site-packages (4.1.1)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /usr/lib/python3.8/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/lib/python3.8/site-packages (from transformers) (4.54.1)\n",
      "Requirement already satisfied: sacremoses in /usr/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /storagenfs/g.pisciotta1/.local/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: joblib in /usr/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /storagenfs/g.pisciotta1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip3 install pytorch\n",
    "!pip3 install multiprocess\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#!pip3 install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator, CEBinaryClassificationEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import multiprocess as multiprocessing\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "import logging\n",
    "!pip3 install transformers\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from writeprints import get_writeprints_transformer, prepare_entry\n",
    "from utills import batch\n",
    "from pytorch_models import NeuralNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import copy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from transformers import BertTokenizerFast\n",
    "import datetime\n",
    "import time\n",
    "from pan20_verif_evaluator import evaluate_all\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = 'data/small/'\n",
    "TEMP_DATA_DIR = 'temp_data/pan20_computed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ground truth\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f884fce5acb4a15b5125d0454d83da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "52601 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# Load gound truth\n",
    "ground_truth = {}\n",
    "partition = {}\n",
    "\n",
    "#tot = 52601/3\n",
    "limited = False\n",
    "tot = 20000\n",
    "n_of_pos_we_want = tot/2\n",
    "n_of_neg_we_want = tot/2\n",
    "\n",
    "# quindi in totale 1_000\n",
    "positive_samples = 0\n",
    "negative_samples = 0\n",
    "total = 0\n",
    "\n",
    "# Qui apriamo il dataset e estraiamo un totale di 10k esempi del task\n",
    "with open(DATA_DIR + '/pan20-authorship-verification-training-small-truth.jsonl', 'r') as f:\n",
    "    \n",
    "    for counter, l in tqdm(enumerate(f)):\n",
    "        total += 1\n",
    "        \n",
    "        d = json.loads(l)\n",
    "\n",
    "        if d['same'] and positive_samples < n_of_pos_we_want : # se la label è true e dobbiamo ancora aggiungerne\n",
    "            ground_truth[d['id']] = d['same']\n",
    "            positive_samples += 1\n",
    "        elif not d['same'] and negative_samples < n_of_neg_we_want:\n",
    "            ground_truth[d['id']] = d['same']\n",
    "            negative_samples += 1\n",
    "            \n",
    "        # Una volta che abbiamo raggiunto il numero di dati che vogliamo, è fatta, usciamo\n",
    "        if limited and (positive_samples == n_of_pos_we_want and negative_samples == n_of_neg_we_want):\n",
    "            break\n",
    "            \n",
    "print(total, positive_samples, negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partially extract features\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started handling of each pair\n",
      "Ended handling of each pair\n",
      "Time spent:  212.726308\n"
     ]
    }
   ],
   "source": [
    "samples = multiprocessing.Manager().Queue()\n",
    "\n",
    "from sklearn.model_selection import train_test_split                 \n",
    "def process_pair(l):\n",
    "    d = json.loads(l)\n",
    "    if d['id'] in ground_truth:\n",
    "        e1 = prepare_entry(d['pair'][0])\n",
    "        e2 = prepare_entry(d['pair'][1])\n",
    "        samples.put({'id': d['id'], 'doc1': e1, 'doc2': e2})\n",
    "        return {'id': d['id'], 'doc1':e1, 'doc2':e2}    \n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"Started handling of each pair\")\n",
    "with open(DATA_DIR + 'pan20-authorship-verification-training-small.jsonl', 'r') as f:\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        samples = pool.map(process_pair, (l for l in f))\n",
    "print(\"Ended handling of each pair\")        \n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Time spent: \", (end_time-start_time).total_seconds())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "# Il processo in parallelo restituisce comunque eventualmente dei None se \n",
    "# il sample non appartiene alla lista di quelli che abbiamo selezionato,\n",
    "# quindi nel caso rimuoviamo i None così\n",
    "samples = [i for i in samples if i is not None]\n",
    "\n",
    "for s in samples:\n",
    "    # Qui non dovrebbe arrivare\n",
    "    if s is None:\n",
    "        print(\"None\")\n",
    "        break\n",
    "        \n",
    "    # Questo dovrebbe essere il caso normale\n",
    "    else:\n",
    "        s['label'] = int(ground_truth[s['id']])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(samples)\n",
    "df = df.set_index('id')\n",
    "\n",
    "X_design, X_test, y_design, y_test = train_test_split(df, df['label'], stratify=df['label'], random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_design, y_design, stratify=y_design, random_state=42)\n",
    "\n",
    "# Per salvare\n",
    "X_test.to_pickle(TEMP_DATA_DIR+\"x_test.pkl\")\n",
    "X_valid.to_pickle(TEMP_DATA_DIR+\"x_valid.pkl\")\n",
    "X_train.to_pickle(TEMP_DATA_DIR+\"x_train.pkl\")\n",
    "df.to_pickle(TEMP_DATA_DIR+\"df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6cced668-6e51-5212-873c-717f2bc91ce6</th>\n",
       "      <td>{'preprocessed': 'I shift a bit, warily lettin...</td>\n",
       "      <td>{'preprocessed': '\"All will become one with Ru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3c6c188a-db28-59aa-8c09-3d0f799ff579</th>\n",
       "      <td>{'preprocessed': 'I shift a bit, warily lettin...</td>\n",
       "      <td>{'preprocessed': 'Suddenly, a piece of ice fal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b0cfa94f-c9ec-5aa5-8331-a5a249b664cf</th>\n",
       "      <td>{'preprocessed': 'A single tear escaped me as ...</td>\n",
       "      <td>{'preprocessed': 'got the Yang yoyo.\" Kimiko p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6e86e73-9a7b-58f2-a652-a17b4a1bcabf</th>\n",
       "      <td>{'preprocessed': '\"Ja.\" Ludwig kept his gaze u...</td>\n",
       "      <td>{'preprocessed': 'SilverGray lll...YellowRagge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4fe541af-912e-5a86-81a5-94c6d3891509</th>\n",
       "      <td>{'preprocessed': 'And he did. Slowly, hesitant...</td>\n",
       "      <td>{'preprocessed': '\"Let\"s go,\" Raimondo said an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d244814f-fbf1-5db4-a40c-b973b08ac9f8</th>\n",
       "      <td>{'preprocessed': 'He had a nice voice. Althoug...</td>\n",
       "      <td>{'preprocessed': 'trusty goose-down pillow. Ra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a04024b3-2fba-5ee3-ab06-dac5c10301b6</th>\n",
       "      <td>{'preprocessed': 'I can be with you until 9 p....</td>\n",
       "      <td>{'preprocessed': '\"She likes him more than you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4c5e5e32-79fe-5239-ae1e-a295ca20f6c0</th>\n",
       "      <td>{'preprocessed': 'To help us most to grow If w...</td>\n",
       "      <td>{'preprocessed': 'Onward to the next chapter, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fb7abb4c-c464-5f4e-9366-a594a231cbdf</th>\n",
       "      <td>{'preprocessed': '\"Amy\"s house?\" Ken gave him ...</td>\n",
       "      <td>{'preprocessed': '\"It\"s nothing. We we\"re just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f3b092e-fd73-5d57-91d2-3dc2282ea569</th>\n",
       "      <td>{'preprocessed': 'I like her. Are you happy no...</td>\n",
       "      <td>{'preprocessed': 'Mort and I have also welcome...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   doc1  \\\n",
       "id                                                                                        \n",
       "6cced668-6e51-5212-873c-717f2bc91ce6  {'preprocessed': 'I shift a bit, warily lettin...   \n",
       "3c6c188a-db28-59aa-8c09-3d0f799ff579  {'preprocessed': 'I shift a bit, warily lettin...   \n",
       "b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  {'preprocessed': 'A single tear escaped me as ...   \n",
       "e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  {'preprocessed': '\"Ja.\" Ludwig kept his gaze u...   \n",
       "4fe541af-912e-5a86-81a5-94c6d3891509  {'preprocessed': 'And he did. Slowly, hesitant...   \n",
       "...                                                                                 ...   \n",
       "d244814f-fbf1-5db4-a40c-b973b08ac9f8  {'preprocessed': 'He had a nice voice. Althoug...   \n",
       "a04024b3-2fba-5ee3-ab06-dac5c10301b6  {'preprocessed': 'I can be with you until 9 p....   \n",
       "4c5e5e32-79fe-5239-ae1e-a295ca20f6c0  {'preprocessed': 'To help us most to grow If w...   \n",
       "fb7abb4c-c464-5f4e-9366-a594a231cbdf  {'preprocessed': '\"Amy\"s house?\" Ken gave him ...   \n",
       "8f3b092e-fd73-5d57-91d2-3dc2282ea569  {'preprocessed': 'I like her. Are you happy no...   \n",
       "\n",
       "                                                                                   doc2  \\\n",
       "id                                                                                        \n",
       "6cced668-6e51-5212-873c-717f2bc91ce6  {'preprocessed': '\"All will become one with Ru...   \n",
       "3c6c188a-db28-59aa-8c09-3d0f799ff579  {'preprocessed': 'Suddenly, a piece of ice fal...   \n",
       "b0cfa94f-c9ec-5aa5-8331-a5a249b664cf  {'preprocessed': 'got the Yang yoyo.\" Kimiko p...   \n",
       "e6e86e73-9a7b-58f2-a652-a17b4a1bcabf  {'preprocessed': 'SilverGray lll...YellowRagge...   \n",
       "4fe541af-912e-5a86-81a5-94c6d3891509  {'preprocessed': '\"Let\"s go,\" Raimondo said an...   \n",
       "...                                                                                 ...   \n",
       "d244814f-fbf1-5db4-a40c-b973b08ac9f8  {'preprocessed': 'trusty goose-down pillow. Ra...   \n",
       "a04024b3-2fba-5ee3-ab06-dac5c10301b6  {'preprocessed': '\"She likes him more than you...   \n",
       "4c5e5e32-79fe-5239-ae1e-a295ca20f6c0  {'preprocessed': 'Onward to the next chapter, ...   \n",
       "fb7abb4c-c464-5f4e-9366-a594a231cbdf  {'preprocessed': '\"It\"s nothing. We we\"re just...   \n",
       "8f3b092e-fd73-5d57-91d2-3dc2282ea569  {'preprocessed': 'Mort and I have also welcome...   \n",
       "\n",
       "                                      label  \n",
       "id                                           \n",
       "6cced668-6e51-5212-873c-717f2bc91ce6      1  \n",
       "3c6c188a-db28-59aa-8c09-3d0f799ff579      1  \n",
       "b0cfa94f-c9ec-5aa5-8331-a5a249b664cf      1  \n",
       "e6e86e73-9a7b-58f2-a652-a17b4a1bcabf      1  \n",
       "4fe541af-912e-5a86-81a5-94c6d3891509      1  \n",
       "...                                     ...  \n",
       "d244814f-fbf1-5db4-a40c-b973b08ac9f8      0  \n",
       "a04024b3-2fba-5ee3-ab06-dac5c10301b6      0  \n",
       "4c5e5e32-79fe-5239-ae1e-a295ca20f6c0      0  \n",
       "fb7abb4c-c464-5f4e-9366-a594a231cbdf      0  \n",
       "8f3b092e-fd73-5d57-91d2-3dc2282ea569      0  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the feature sklearn transformer \n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py:688: UserWarning:\n",
      "\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  2192.447666\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "docs = list(X_train['doc1']) + list(X_train['doc2'])\n",
    "transformer = get_writeprints_transformer()\n",
    "X = transformer.fit_transform(docs[:len(docs)//2]) # Usiamo 1/2 dei docs per fittare, valori più alti fanno esplodere\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "with open(TEMP_DATA_DIR + 'transformers.p', 'wb') as f:\n",
    "    pickle.dump((transformer, scaler), f)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Time: \", (end-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the training data\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  4108.768206\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "x1 = scaler.transform(transformer.transform(X_train['doc1']))\n",
    "x2 = scaler.transform(transformer.transform(X_train['doc2']))\n",
    "\n",
    "X_train_features = pd.DataFrame(np.abs(x1-x2).todense())\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Time: \", (end-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the test data\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1900.568006\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "x1 = scaler.transform(transformer.transform(X_test['doc1']))\n",
    "x2 = scaler.transform(transformer.transform(X_test['doc2']))\n",
    "\n",
    "X_test_features = pd.DataFrame(np.abs(x1-x2).todense())\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Time: \", (end-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the Val data\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1433.722064\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "        \n",
    "x1 = scaler.transform(transformer.transform(X_valid['doc1']))\n",
    "x2 = scaler.transform(transformer.transform(X_valid['doc2']))\n",
    "X_valid_features = pd.DataFrame(np.abs(x1-x2).todense())\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Time: \", (end-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to (or read from) pickle files\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp_data/pan20_computed/ordering_metadata.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-780a92413411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEMP_DATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ordering_metadata.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtrain_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'temp_data/pan20_computed/ordering_metadata.p'"
     ]
    }
   ],
   "source": [
    "import pickle, pandas as pd\n",
    "\"\"\"\n",
    "# Per salvare\n",
    "X_test_features.to_pickle(TEMP_DATA_DIR+\"x_test_features.pkl\")\n",
    "X_valid_features.to_pickle(TEMP_DATA_DIR+\"x_valid_features.pkl\")\n",
    "X_train_features.to_pickle(TEMP_DATA_DIR+\"x_train_features.pkl\")\n",
    "\n",
    "X_test.to_pickle(TEMP_DATA_DIR+\"x_test.pkl\")\n",
    "X_valid.to_pickle(TEMP_DATA_DIR+\"x_valid.pkl\")\n",
    "X_train.to_pickle(TEMP_DATA_DIR+\"x_train.pkl\")\n",
    "\n",
    "df.to_pickle(TEMP_DATA_DIR+\"df.pkl\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Per leggere quelli già salvati\n",
    "X_test_features = pd.read_pickle(TEMP_DATA_DIR+\"/x_test_features.pkl\")\n",
    "X_valid_features = pd.read_pickle(TEMP_DATA_DIR+\"/x_valid_features.pkl\")\n",
    "X_train_features = pd.read_pickle(TEMP_DATA_DIR+\"/x_train_features.pkl\")\n",
    "\n",
    "X_test = pd.read_pickle(TEMP_DATA_DIR+\"/x_test.pkl\")\n",
    "X_valid = pd.read_pickle(TEMP_DATA_DIR+\"/x_valid.pkl\")\n",
    "X_train = pd.read_pickle(TEMP_DATA_DIR+\"/x_train.pkl\")\n",
    "\n",
    "df = pd.read_pickle(TEMP_DATA_DIR+\"/df.pkl\")\n",
    "y_train = X_train[['label']]\n",
    "y_valid = X_valid[['label']]\n",
    "y_test = X_test[['label']]\n",
    "\n",
    "with open(TEMP_DATA_DIR + 'transformers.p', 'rb') as f:\n",
    "    transformer, scaler = pickle.load(f)\n",
    "    \n",
    "with open(TEMP_DATA_DIR + 'ordering_metadata.p', 'rb') as f:\n",
    "    train_sz, test_sz, val_sz, train_idxs = pickle.load(f)\n",
    "    \n",
    "with open(TEMP_DATA_DIR + 'ordering_metadata.p', 'rb') as f:\n",
    "    train_idxs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Classifier: Logistic Regression\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b6e4bcd6-d5e5-583e-a575-32ccdabae79c</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501c2a19-9a64-5c74-9a1d-447508791eb2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74e4dc00-12ff-5086-b0df-b6d2997605ee</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ce6b0f7a-2506-59be-a8b4-5aa58aa1aca8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ee4f1241-8bea-53c3-935e-4c6727fac04e</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ce50255f-a919-5513-9a41-366839dc33ad</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e05d60d5-9bb8-5f5d-b0f3-74a9ecab514e</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4a569b1b-4ff8-54d4-9a57-0e378da68b0c</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6ac3857-dfbd-5578-9143-bc210f461a18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e152c2f4-2cb3-5a21-95fb-ebcb06eaef08</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      label\n",
       "id                                         \n",
       "b6e4bcd6-d5e5-583e-a575-32ccdabae79c      0\n",
       "501c2a19-9a64-5c74-9a1d-447508791eb2      0\n",
       "74e4dc00-12ff-5086-b0df-b6d2997605ee      1\n",
       "ce6b0f7a-2506-59be-a8b4-5aa58aa1aca8      1\n",
       "ee4f1241-8bea-53c3-935e-4c6727fac04e      1\n",
       "...                                     ...\n",
       "ce50255f-a919-5513-9a41-366839dc33ad      0\n",
       "e05d60d5-9bb8-5f5d-b0f3-74a9ecab514e      0\n",
       "4a569b1b-4ff8-54d4-9a57-0e378da68b0c      0\n",
       "d6ac3857-dfbd-5578-9143-bc210f461a18      1\n",
       "e152c2f4-2cb3-5a21-95fb-ebcb06eaef08      1\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   36.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.919, 'c@1': 0.919, 'f_05_u': 0.917, 'F1': 0.919, 'overall': 0.918}\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42, verbose=1).fit(X_train_features, y_train)\n",
    "y_pred = clf.predict(X_test_features)\n",
    "y_test_new = []\n",
    "for i, row in y_test.iterrows():\n",
    "    y_test_new.append(row.values[0])\n",
    "y_test_new = np.array(y_test_new)\n",
    "#y_test = y_test_new\n",
    "print(evaluate_all(y_test_new, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.28083534\n",
      "Iteration 2, loss = 0.69667992\n",
      "Iteration 3, loss = 0.69574730\n",
      "Iteration 4, loss = 0.69499425\n",
      "Iteration 5, loss = 0.69442011\n",
      "Iteration 6, loss = 0.69400266\n",
      "Iteration 7, loss = 0.69370659\n",
      "Iteration 8, loss = 0.69352375\n",
      "Iteration 9, loss = 0.69341281\n",
      "Iteration 10, loss = 0.69334811\n",
      "Iteration 11, loss = 0.69330076\n",
      "Iteration 12, loss = 0.69327544\n",
      "Iteration 13, loss = 0.69325528\n",
      "Iteration 14, loss = 0.69326019\n",
      "Iteration 15, loss = 0.69324977\n",
      "Iteration 16, loss = 0.69324759\n",
      "Iteration 17, loss = 0.69324540\n",
      "Iteration 18, loss = 0.69324772\n",
      "Iteration 19, loss = 0.69324204\n",
      "Iteration 20, loss = 0.69324063\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "{'auc': 0.5, 'c@1': 0.5, 'f_05_u': 0.003, 'F1': 0.001, 'overall': 0.251}\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=42, hidden_layer_sizes=100, activation='relu', solver='adam', learning_rate='constant', max_iter=20, verbose=1).fit(X_train_features, y_train.values.ravel())\n",
    "y_pred = clf.predict(X_valid_features)\n",
    "y_val_new = []\n",
    "for i, row in y_valid.iterrows():\n",
    "    y_val_new.append(row.values[0])\n",
    "y_val_new = np.array(y_val_new)\n",
    "#y_test = y_test_new\n",
    "print(evaluate_all(y_val_new, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.886, 'c@1': 0.886, 'f_05_u': 0.89, 'F1': 0.884, 'overall': 0.886}\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=42, hidden_layer_sizes=100, activation='logistic', solver='lbfgs', learning_rate='constant', max_iter=20, verbose=1).fit(X_train_features, y_train.values.ravel())\n",
    "y_pred = clf.predict(X_valid_features)\n",
    "y_val_new = []\n",
    "for i, row in y_valid.iterrows():\n",
    "    y_val_new.append(row.values[0])\n",
    "y_val_new = np.array(y_val_new)\n",
    "#y_test = y_test_new\n",
    "print(evaluate_all(y_val_new, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.907, 'c@1': 0.907, 'f_05_u': 0.905, 'F1': 0.908, 'overall': 0.907}\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=42, hidden_layer_sizes=200, activation='logistic', solver='lbfgs', learning_rate='invscaling', max_iter=50, verbose=1).fit(X_train_features, y_train.values.ravel())\n",
    "y_pred = clf.predict(X_valid_features)\n",
    "y_val_new = []\n",
    "for i, row in y_valid.iterrows():\n",
    "    y_val_new.append(row.values[0])\n",
    "y_val_new = np.array(y_val_new)\n",
    "#y_test = y_test_new\n",
    "print(evaluate_all(y_val_new, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.916, 'c@1': 0.916, 'f_05_u': 0.916, 'F1': 0.917, 'overall': 0.916}\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_features)\n",
    "\n",
    "y_test_new = []\n",
    "for i, row in y_test.iterrows():\n",
    "    y_test_new.append(row.values[0])\n",
    "y_test_new = np.array(y_test_new)\n",
    "#y_test = y_test_new\n",
    "print(evaluate_all(y_test_new, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
